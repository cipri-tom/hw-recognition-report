%!TEX root = ../main.tex

\chapter{Transcription}
\label{ch:transcription}
This chapter is dedicated to the problem of offine handwriting recognition, that is, given an image of a text line, find the corresponding characters and words. Although some may consider it a solved task, since many approaches achieve very good performance on a wide variety of corpora (see \autoref{sec:related_transcription}), we remind the reader that our use case poses several challenges that are not present in the controlled environments of academic contests (\autoref{sec:challenges}). Of these, we restate the most important ones:
\begin{description}
	\item[unbefitting writing conditions] The documents are completed in a rush, soon after an accident, with no proper support structure for writting and without restrictions with regards to content.

	\item[lack of annotated data] As in the case of detection, we started the project having only a database of images and no useful annotations.
\end{description}

The chapter first introduces the \CRNN{} architecture \citep{CRNN}, which has become almost a standard for optical character recognition in the recent years. Then, \autoref{sec:transcription_experiments} presents several experiments for training, including various methods of data generation. Each new experiment tries to address the weaknesses of the previous one, in a quest of reproducing the great results obtained on clean databases. Finally, we explore the potential of an attention-based architecture coupled with the same training data in \autoref{sec:attention} and present an overview of the obtained results in \autoref{sec:transcription_results}.


\startToDo{}

%========================================================================================

\section{Convolutional Recurrent Neural Networks}\label{sec:crnn}

	%----------------------------------------------------------------------------------------

	\subsection{Feature extraction}

	%----------------------------------------------------------------------------------------

	\subsection{RNN decoder}

	%----------------------------------------------------------------------------------------

	\subsection{Training}
		dropout

		alphabet encoding


	%----------------------------------------------------------------------------------------

	\subsection{Evaluation}
		edit distance

		lowercase, uppercase

		mention the TEST corpus and the thing with underscores

%========================================================================================

\section{Experiments}\label{sec:transcription_experiments}

	%----------------------------------------------------------------------------------------

	\subsection{Academic databases}
		RIMES lines, words, words with spaces + IAM

		show examples, and numbers

		Mention problems: writing style, different corpus (MAJ vs min vs ligatures + numbers, plates), lines

	%----------------------------------------------------------------------------------------

	\subsection{Generator}

		mention the success in OCR

		mention specific changes that we brought (no projection, no curved, random jitter, lines)

		subsubsection: scrapping fonts + elastic distortions

		results:  closer, but not enough doesn't know if it's a date or a number so D, 0, O

	%----------------------------------------------------------------------------------------

	\subsection{Corpus type}
		strategy 1: precondition the hidden state, like Karpathy
		Not working because not enough information. They were embedding a very rich feature into the space of the hidden state, whereas we merely ``tickle'' the hiddent state

		Strategy 2: add extra feature at all time steps

%========================================================================================

\section{Attention-based architecture}\label{sec:attention}
	biiig TODO

%========================================================================================

\section{Results}\label{sec:transcription_results}

		Show a table of each approach and the corresponding CER, and call it a day :)

\stopToDo{}

