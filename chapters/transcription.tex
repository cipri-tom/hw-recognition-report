%!TEX root = ../main.tex

\chapter{Transcription}\label{ch:transcription}

This chapter is dedicated to the problem of offine handwriting recognition, that is, given an image of a text line, find the corresponding characters and words. Although some may consider it a solved task, since many approaches achieve very good performance on a wide variety of corpora (see \autoref{sec:related_transcription}), we remind the reader that our use case poses several challenges that are not present in the controlled environments of academic contests (\autoref{sec:challenges}). Of these, we summarise the most important ones:
\begin{description}
	\item[unbefitting writing conditions] The documents are completed in a rush, soon after an accident, with no proper support structure for writting and with few format restrictions.

	\item[unconstrained recognition] Statements include text which does not admit a vocabulary for correction, such as phone numbers and license plates.

	\item[lack of annotated data] As in the case of detection, we started the project having only a database of images and no useful annotations.
\end{description}

The chapter first introduces the \CRNN{} architecture \citep{CRNN}, which has become almost a standard for optical character recognition in the recent years. Then, \autoref{sec:transcription_experiments} presents several experiments for training, including various methods of data generation. Each new experiment tries to address the weaknesses of the previous one, in a quest of reproducing the great results obtained on clean databases. Finally, we explore the potential of an attention-based architecture coupled with the same training data in \autoref{sec:attention} and present an overview of the obtained results in \autoref{sec:transcription_results}.



%========================================================================================

\section{Convolutional Recurrent Neural Networks}\label{sec:crnn}

	%--INTRO --------------------------------------------------------------------------------

		Text transcription has preoccupied researchers for a very long time. Therefore, a large amount of ideas have been tried in order to solve this problem, of which we mention a few in \autoref{sec:related_transcription}. One of the challenges of text resides its wide variation of appearance. For example, a free font website such as \url{http://www.1001fonts.com/} currently lists approximately 9500 different typefaces. Moreover, it is believed that each person has a completely unique handwriting, which is why this is still commonly used a signature. This highlights the importance of being able to represent images of text robustly, or, in other words, to extract good features that distinguish well among characters. As was previously stated, convolutional neural networks have proved very effective in this task time and again, so it is natural to use them as a first stage in a transcription architecture.

		Another peculiarity of text, which we have also mentioned in the detection chapter, is given by its sequential aspect. Few neural network architectures support arbitrarily-sized input. Of these, the recurrent neural network (RNN) has become a standard since it models well sequences of any type.

		The Convolutional Recurrent Neural Network (\CRNN{}) architecture uses the two approaches above in a unified framework to transcribe text from natural images. Considering that it achieved state-of-the-art performance in such a challenging task, we believe it is suitable for our use case as well, which is why we focused most of our efforts in this direction. Next, we detail its inner workings as they are crucial in devising our experiments.


		\begin{figure}\begin{tabular}{|c|c|}
			% \footnotesize
			\hline
			\textbf{Type} & \textbf{Configurations}						\tabularnewline	\hline
																																				\hline
			Transcription & - 																\tabularnewline	\hline
			Bidirectional-LSTM & \#hidden units:256						\tabularnewline	\hline
			Bidirectional-LSTM & \#hidden units:256						\tabularnewline	\hline
			Map-to-Sequence & - 															\tabularnewline	\hline
			BatchNormalization & - 														\tabularnewline	\hline
			Convolution & \#maps:512, k:$2\times2$, s:1, p:0	\tabularnewline	\hline
			MaxPooling & Window:$1\times2$, s:2								\tabularnewline	\hline
			Convolution & \#maps:512, k:$3\times3$, s:1, p:1	\tabularnewline	\hline
			BatchNormalization & - 														\tabularnewline	\hline
			Convolution & \#maps:512, k:$3\times3$, s:1, p:1	\tabularnewline	\hline
			MaxPooling & Window:$1\times2$, s:2 							\tabularnewline	\hline
			Convolution & \#maps:256, k:$3\times3$, s:1, p:1	\tabularnewline	\hline
			BatchNormalization & - 														\tabularnewline	\hline
			Convolution & \#maps:256, k:$3\times3$, s:1, p:1	\tabularnewline	\hline
			MaxPooling & Window:$2\times2$, s:2								\tabularnewline	\hline
			Convolution & \#maps:128, k:$3\times3$, s:1, p:1	\tabularnewline	\hline
			MaxPooling & Window:$2\times2$, s:2 							\tabularnewline	\hline
			Convolution & \#maps:64, k:$3\times3$, s:1, p:1		\tabularnewline	\hline
			Input & $W\times32$ gray-scale image 							\tabularnewline	\hline
		\end{tabular}\par
		\caption[\CRNN{} architecture]{\todo{Note different from paper}}\label{fig:crnn_architecture}
		\end{figure}

	%----------------------------------------------------------------------------------------
	\subsection{Feature extraction}
		The first part of the \CRNN{} architecture consists of a custom 7-layer CNN that transforms the input image into a sequence of high-level features. This is similar to well-known architectures, as it alternates convolutional and pooling layers (\autoref{fig:crnn_architecture}). A few particularities are important to be noted.

		First, the input is pooled differently on the vertical dimension than on the horizontal one. For the former, 4 windows of height 2 are used. As a result, an input of height 32 becomes a feature map of height 2. In order to feed this to the RNN, the features on the same column are concatenated into a single vector. Then, the nework performs horizontal pooling only twice with a window of width 2. This ensures the receptive field does not become too wide and can still distinguish very narrow characters such as \{\texttt{l,i,1}\}. The network is fully convolutional, which allows inputs of any size to be processed. However, since the columns become features of the RNN, the height should be standardised. This design is well suited for images of text which are bounded vertically, but not horizontally.

		Second, the network uses batch normalisation \citep{batch_norm}, which is standard practice in recent times since it improves the performance and the stability of the network. This works by replacing the activations \(\ve{H}\) of a layer with their normalised counterparts \(\ve{H}'\), which become input for the next layer: \[
			\ve{H}' = \gamma \frac{\ve{H} - \ve{\mu}}{\ve{\sigma}} + \beta,
		\]\footnote{\todo{make miu and sigma vectors}} where \(\mu\) is a vector of means of all neurons and \(\sigma\) a vector of their standard deviations. The learnable parameters \(\gamma\) and \(\beta\) ensure the network does not lose expressive power by allowing convergence towards any mean and standard deviation. Overall, this also means that the composition of the batches is important, as we will see shortly.

		Finally, we use ReLU activations at each layer and bias terms for the convolution.

		%........................................................................................
		\subsubsection*{Input preprocessing}

			Input images are resized to have a heigth of 32px, while keeping their aspect ratio. As a result, their widths are different which makes it difficult to construct batch tensors. Therefore, we also define a standard input width of 250px and make sure all training images are narrower than this in the interest of avoiding horizontal deformation. Images that are shorter than the necessary width are duplicated horizontally up to this length. As opposed to zero padding the shorter images, this mechanism keeps a similar distribution of pixels across all batches. Note this does not produce bad training examples as we keep track of the original length when aligning the predictions and the labels.

\startToDo{}

	%----------------------------------------------------------------------------------------

	\subsection{RNN decoder}

	%----------------------------------------------------------------------------------------

	\subsection{Training}
		dropout

		alphabet encoding


	%----------------------------------------------------------------------------------------

	\subsection{Evaluation}
		edit distance

		lowercase, uppercase

		mention the TEST corpus and the thing with underscores

%========================================================================================

\section{Experiments}\label{sec:transcription_experiments}

	%----------------------------------------------------------------------------------------

	\subsection{Academic databases}
		RIMES lines, words, words with spaces + IAM

		show examples, and numbers

		Mention problems: writing style, different corpus (MAJ vs min vs ligatures + numbers, plates), lines

	%----------------------------------------------------------------------------------------

	\subsection{Generator}

		mention the success in OCR

		mention specific changes that we brought (no projection, no curved, random jitter, lines)

		subsubsection: scrapping fonts + elastic distortions

		results:  closer, but not enough doesn't know if it's a date or a number so D, 0, O

	%----------------------------------------------------------------------------------------

	\subsection{Corpus type}
		strategy 1: precondition the hidden state, like Karpathy
		Not working because not enough information. They were embedding a very rich feature into the space of the hidden state, whereas we merely ``tickle'' the hiddent state

		Strategy 2: add extra feature at all time steps

%========================================================================================

\section{Attention-based architecture}\label{sec:attention}
	biiig TODO

%========================================================================================

\section{Results}\label{sec:transcription_results}

		Show a table of each approach and the corresponding CER, and call it a day :)

\stopToDo{}

